{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a32af-2e61-4a59-bcd4-7e2abe01b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re,time,warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict,List,Optional,Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import networkx as nx\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from stable_baselines3 import PPO\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def _s(q=42):\n",
    "    np.random.seed(q);torch.manual_seed(q)\n",
    "    if torch.cuda.is_available():torch.cuda.manual_seed_all(q)\n",
    "\n",
    "def _ndc(x)->Optional[int]:\n",
    "    if pd.isna(x):return None\n",
    "    m=re.search(r\"(\\d+)\",str(x))\n",
    "    return int(m.group(1))if m else None\n",
    "\n",
    "def _mrc(v:Optional[int])->str:\n",
    "    if v is None:return\"Unknown\"\n",
    "    if 1<=v<=26:return\"Internal Failure\"\n",
    "    if 27<=v<=39:return\"Damage\"\n",
    "    if 40<=v<=77:return\"Poor Connections\"\n",
    "    if 78<=v<=102:return\"Other\"\n",
    "    return\"Unknown\"\n",
    "\n",
    "def _rdc(df:pd.DataFrame)->pd.DataFrame:\n",
    "    h={};kc=[]\n",
    "    for c in df.columns:\n",
    "        ch=pd.util.hash_pandas_object(df[c],index=False).sum()\n",
    "        if ch not in h:h[ch]=c;kc.append(c)\n",
    "        elif not df[c].equals(df[h[ch]]):kc.append(c)\n",
    "    return df[kc]\n",
    "\n",
    "def _mi(df:pd.DataFrame)->pd.DataFrame:\n",
    "    o=df.copy()\n",
    "    for c in o.columns:\n",
    "        if o[c].isna().any():\n",
    "            mv=o[c].mode(dropna=True);fv=mv.iloc[0]if len(mv)else\"Unknown\";o[c]=o[c].fillna(fv)\n",
    "    return o\n",
    "\n",
    "def _lpp(fp:str,tc:str=\"Root Cause\",dcc:str=\"Defect Code\",fc:Optional[List[str]]=None)->Tuple[np.ndarray,List[LabelEncoder],List[str],Dict]:\n",
    "    if not os.path.exists(fp):raise FileNotFoundError(f\"Data file not found: {fp}\")\n",
    "    df=pd.read_excel(fp)\n",
    "    if tc not in df.columns:df[tc]=np.nan\n",
    "    if dcc in df.columns:\n",
    "        dci=df[dcc].apply(_ndc);mp=dci.apply(_mrc)\n",
    "        if df[tc].isna().any():df[tc]=df[tc].fillna(mp)\n",
    "        else:df[tc]=df[tc].replace(\"\",np.nan).fillna(mp)\n",
    "    if fc is None:\n",
    "        dl={\"id\",\"index\",\"timestamp\",\"time\",\"date\"};fc=[c for c in df.columns if c!=tc and c.lower()not in dl]\n",
    "    ms=[c for c in fc+[tc]if c not in df.columns]\n",
    "    if ms:raise ValueError(f\"Missing required columns: {ms}\")\n",
    "    df=df[fc+[tc]].copy();df=_rdc(df)\n",
    "    cc=[c for c in df.columns if df[c].nunique(dropna=False)<=1 and c!=tc]\n",
    "    if cc:df=df.drop(columns=cc)\n",
    "    fc=[c for c in fc if c in df.columns and c!=tc]\n",
    "    df=_mi(df);enc:List[LabelEncoder]=[];de=np.zeros(df.shape,dtype=np.int64)\n",
    "    for j,c in enumerate(df.columns):le=LabelEncoder();de[:,j]=le.fit_transform(df[c].astype(str));enc.append(le)\n",
    "    fn=[c for c in df.columns if c!=tc];d=de.shape[1];N=de.shape[0];tcl=enc[-1].classes_.tolist()\n",
    "    mt={\"N\":N,\"d\":d,\"n_features\":d-1,\"feature_names\":fn,\"target_col\":tc,\"target_classes\":tcl,\"columns\":df.columns.tolist()}\n",
    "    return de,enc,fn,mt\n",
    "\n",
    "def _bdc(fn:List[str],d:int)->Tuple[np.ndarray,np.ndarray]:\n",
    "    C=np.ones((d,d),dtype=np.int8);E=np.zeros((d,d),dtype=np.int8);np.fill_diagonal(C,0)\n",
    "    ri=d-1;C[ri,:]=0;n2i={n.lower().replace(\" \",\"_\"):i for i,n in enumerate(fn)}\n",
    "    def _gi(t:str)->Optional[int]:\n",
    "        t=t.lower().replace(\" \",\"_\")\n",
    "        for k,v in n2i.items():\n",
    "            if t in k:return v\n",
    "        return None\n",
    "    ps,ds,os,pr,psr,pt,spt,mt,mst=_gi(\"production_stage\"),_gi(\"destination_stage\"),_gi(\"original_production_stage\"),_gi(\"process\"),_gi(\"product_series\"),_gi(\"product_type\"),_gi(\"subproduct_type\"),_gi(\"memory_type\"),_gi(\"memory_subtype\")\n",
    "    if ps is not None and ds is not None:C[ds,ps]=0;E[ps,ds]=1\n",
    "    if os is not None and ps is not None:C[ps,os]=0;E[os,ps]=1\n",
    "    if pr is not None and ps is not None:C[ps,pr]=0;E[pr,ps]=1\n",
    "    if psr is not None and pt is not None:C[pt,psr]=0;E[psr,pt]=1\n",
    "    if pt is not None and spt is not None:C[spt,pt]=0;E[pt,spt]=1\n",
    "    if mt is not None and mst is not None:C[mst,mt]=0;E[mt,mst]=1\n",
    "    return C,E\n",
    "\n",
    "class _LDM(nn.Module):\n",
    "    def __init__(self,d:int,C:np.ndarray,E:np.ndarray,sd:int=42):\n",
    "        super().__init__();self.d=d;dv=torch.device(\"cuda\"if torch.cuda.is_available()else\"cpu\")\n",
    "        Ct=torch.from_numpy(C).float().to(dv);Et=torch.from_numpy(E).float().to(dv);It=torch.eye(d,device=dv)\n",
    "        g=torch.Generator(device=dv).manual_seed(sd);ti=0.01*torch.randn((d,d),generator=g,device=dv);ti=ti*Ct\n",
    "        self.ThetaM=nn.Parameter(ti);self.register_buffer(\"C\",Ct);self.register_buffer(\"E\",Et);self.register_buffer(\"I\",It)\n",
    "    def _mr(self)->torch.Tensor:return torch.sigmoid(self.ThetaM)*self.C*(1.0-self.I)\n",
    "    def _muo(self,lp:float)->torch.Tensor:\n",
    "        M=torch.sigmoid(self.ThetaM)*self.C;al=self.C*(1.0-self.I)\n",
    "        Ls=(M*al).sum()/(al.sum()+1e-12);Lp=lp*torch.mean(self.E*(1.0-M)**2);return Ls+Lp\n",
    "    @torch.no_grad()\n",
    "    def _zfg(self)->None:\n",
    "        if self.ThetaM.grad is not None:self.ThetaM.grad*=self.C\n",
    "\n",
    "def _v2ds(z:np.ndarray,M:np.ndarray,d:int)->np.ndarray:\n",
    "    p=z[:d];Ev=z[d:];Em=np.zeros((d,d),dtype=np.float32);iu=np.triu_indices(d,1);Em[iu]=Ev.astype(np.float32)\n",
    "    Ef=Em+Em.T;pd=p[:,None]-p[None,:];sE=1.0/(1.0+np.exp(-Ef));sP=1.0/(1.0+np.exp(pd))\n",
    "    As=sE*sP*M;np.fill_diagonal(As,0.0);return As\n",
    "\n",
    "def _ba(As:np.ndarray,tau:float=0.5)->np.ndarray:A=(As>tau).astype(np.int8);np.fill_diagonal(A,0);return A\n",
    "\n",
    "def _ic(A:np.ndarray)->bool:return not nx.is_directed_acyclic_graph(nx.DiGraph(A))\n",
    "\n",
    "def _ohe():\n",
    "    try:return OneHotEncoder(handle_unknown=\"ignore\",sparse_output=True)\n",
    "    except TypeError:return OneHotEncoder(handle_unknown=\"ignore\",sparse=True)\n",
    "\n",
    "def _fsl(X:np.ndarray,y:np.ndarray)->Pipeline:\n",
    "    nc=len(np.unique(y))\n",
    "    if nc<=1:raise ValueError(\"Target has <=1 class\")\n",
    "    if nc==2:lr=LogisticRegression(max_iter=500,solver=\"liblinear\",C=1.0,random_state=42)\n",
    "    else:lr=LogisticRegression(max_iter=500,solver=\"lbfgs\",multi_class=\"multinomial\",C=1.0,random_state=42)\n",
    "    pp=Pipeline([(\"oh\",_ohe()),(\"lr\",lr)]);pp.fit(X,y);return pp\n",
    "\n",
    "def _llfm(m:Pipeline,X:np.ndarray,y:np.ndarray)->float:\n",
    "    pb=m.predict_proba(X);pb=np.maximum(pb,1e-12);return float(np.sum(np.log(pb[np.arange(len(y)),y])))\n",
    "\n",
    "def _pcfm(m:Pipeline)->int:lr=m.named_steps[\"lr\"];return int(lr.coef_.size+lr.intercept_.size)\n",
    "\n",
    "def _cbas(A:np.ndarray,dt:np.ndarray)->Tuple[float,Dict[int,Tuple[Pipeline,np.ndarray]],List[int]]:\n",
    "    N,d=dt.shape;LL=0.0;kp=0;sms:Dict[int,Tuple[Pipeline,np.ndarray]]={}\n",
    "    for i in range(d):\n",
    "        pa=np.where(A[:,i]==1)[0];y=dt[:,i];nc=len(np.unique(y))\n",
    "        if nc<=1:continue\n",
    "        if len(pa)==0:\n",
    "            for c in range(nc):pc=np.mean(y==c);pc=max(pc,1e-12);LL+=float(np.sum(y==c)*np.log(pc))\n",
    "            kp+=(nc-1)\n",
    "        else:X=dt[:,pa];md=_fsl(X,y);LL+=_llfm(md,X,y);kp+=_pcfm(md);sms[i]=(md,pa)\n",
    "    bc=-2.0*LL+kp*np.log(N);G=nx.DiGraph(A);to=list(nx.topological_sort(G))if nx.is_directed_acyclic_graph(G)else list(range(d))\n",
    "    return float(bc),sms,to\n",
    "\n",
    "@dataclass\n",
    "class _CFC:\n",
    "    max_samples:Optional[int]=None\n",
    "    max_values_per_feature:Optional[int]=None\n",
    "    seed:int=42\n",
    "\n",
    "def _cca(A:np.ndarray,dt:np.ndarray,sms:Dict[int,Tuple[Pipeline,np.ndarray]],to:List[int],cfg:_CFC=_CFC())->float:\n",
    "    rng=np.random.default_rng(cfg.seed);N,d=dt.shape;ti=d-1\n",
    "    if ti not in sms:return 0.0\n",
    "    uv=[np.unique(dt[:,j])for j in range(d)]\n",
    "    if cfg.max_values_per_feature is not None:uv=[(vs if len(vs)<=cfg.max_values_per_feature else rng.choice(vs,size=cfg.max_values_per_feature,replace=False))for vs in uv]\n",
    "    idx=np.arange(N)\n",
    "    if cfg.max_samples is not None and cfg.max_samples<N:idx=rng.choice(idx,size=cfg.max_samples,replace=False)\n",
    "    tt=0;mt=0\n",
    "    for n in idx:\n",
    "        x=dt[n].copy();oy=int(x[ti])\n",
    "        for i in range(d-1):\n",
    "            ov=x[i]\n",
    "            for al in uv[i]:\n",
    "                if al==ov:continue\n",
    "                xcf=x.copy();xcf[i]=int(al);vl=True\n",
    "                for nd in to:\n",
    "                    if nd==i:continue\n",
    "                    if nd in sms:\n",
    "                        md,pa=sms[nd]\n",
    "                        try:pr=int(md.predict(xcf[pa].reshape(1,-1))[0]);xcf[nd]=pr\n",
    "                        except:vl=False;break\n",
    "                if not vl:continue\n",
    "                tt+=1\n",
    "                if int(xcf[ti])==oy:mt+=1\n",
    "    return float(mt/tt)if tt>0 else 0.0\n",
    "\n",
    "def _ccfa(A:np.ndarray,dt:np.ndarray,enc:List[LabelEncoder],fn:List[str],sms:Dict[int,Tuple[Pipeline,np.ndarray]],to:List[int],cfg:_CFC=_CFC())->np.ndarray:\n",
    "    rng=np.random.default_rng(cfg.seed);N,d=dt.shape;ti=d-1;nf=d-1;cls=enc[-1].classes_;ncl=len(cls)\n",
    "    if ti not in sms:return np.zeros((nf,ncl),dtype=np.float32)\n",
    "    uv=[np.unique(dt[:,j])for j in range(d)]\n",
    "    if cfg.max_values_per_feature is not None:uv=[(vs if len(vs)<=cfg.max_values_per_feature else rng.choice(vs,size=cfg.max_values_per_feature,replace=False))for vs in uv]\n",
    "    idx=np.arange(N)\n",
    "    if cfg.max_samples is not None and cfg.max_samples<N:idx=rng.choice(idx,size=cfg.max_samples,replace=False)\n",
    "    at=np.zeros((nf,ncl),dtype=np.float32)\n",
    "    for c in range(ncl):\n",
    "        cs=[n for n in idx if int(dt[n,ti])==c]\n",
    "        if not cs:continue\n",
    "        for i in range(nf):\n",
    "            tt=0;st=0\n",
    "            for n in cs:\n",
    "                x=dt[n].copy();ov=x[i]\n",
    "                for al in uv[i]:\n",
    "                    if al==ov:continue\n",
    "                    xcf=x.copy();xcf[i]=int(al);vl=True\n",
    "                    for nd in to:\n",
    "                        if nd==i:continue\n",
    "                        if nd in sms:\n",
    "                            md,pa=sms[nd]\n",
    "                            try:pr=int(md.predict(xcf[pa].reshape(1,-1))[0]);xcf[nd]=pr\n",
    "                            except:vl=False;break\n",
    "                    if not vl:continue\n",
    "                    tt+=1\n",
    "                    if int(xcf[ti])==c:st+=1\n",
    "            s=(st/tt)if tt>0 else 0.0;at[i,c]=float(-15.0+23.0*s)\n",
    "    return np.clip(at,-15.0,8.0)\n",
    "\n",
    "class _CEF(gym.Env):\n",
    "    metadata={\"render_modes\":[\"human\"]}\n",
    "    def __init__(self,d:int,dc:np.ndarray,mm:_LDM,al:float=0.02,bt:float=10.0,tau:float=0.5,zc:float=10.0,ac:float=0.5,cfc:Optional[_CFC]=None,cs:int=256,sd:int=42):\n",
    "        super().__init__();self.d=d;self.dc=dc;self.mm=mm;self.al=al;self.bt=bt;self.tau=tau;self.zc=zc;self.ac=ac;self.cfc=cfc or _CFC();self.rng=np.random.default_rng(sd)\n",
    "        self.od=d+(d*(d-1))//2;self.action_space=spaces.Box(low=-ac,high=ac,shape=(self.od,),dtype=np.float32);self.observation_space=spaces.Box(low=-zc,high=zc,shape=(self.od,),dtype=np.float32)\n",
    "        self.z=np.zeros((self.od,),dtype=np.float32);self._ch:Dict[Tuple[int,...],Dict]={};self._co:List[Tuple[int,...]]=[];self._cs=cs\n",
    "    def reset(self,seed:Optional[int]=None,options:Optional[dict]=None):\n",
    "        super().reset(seed=seed);self.z=(0.01*self.rng.standard_normal(self.od)).astype(np.float32);return self.z.copy(),{}\n",
    "    def _cg(self,k):return self._ch.get(k,None)\n",
    "    def _cp(self,k,v):\n",
    "        if k in self._ch:self._ch[k]=v;return\n",
    "        self._ch[k]=v;self._co.append(k)\n",
    "        if len(self._co)>self._cs:ol=self._co.pop(0);self._ch.pop(ol,None)\n",
    "    def step(self,act:np.ndarray):\n",
    "        act=np.clip(act,-self.ac,self.ac).astype(np.float32);self.z=np.clip(self.z+act,-self.zc,self.zc)\n",
    "        with torch.no_grad():M=self.mm._mr().detach().cpu().numpy()\n",
    "        As=_v2ds(self.z,M,self.d);A=_ba(As,tau=self.tau)\n",
    "        if _ic(A):rw=-1e6;inf={\"cyclic\":True,\"edges\":int(A.sum())}\n",
    "        else:\n",
    "            ky=tuple(A.flatten().tolist());cd=self._cg(ky)\n",
    "            if cd is None:\n",
    "                bc,sms,to=_cbas(A,self.dc);rs=-self.al*float(A.sum());ca=_cca(A,self.dc,sms,to,cfg=self.cfc);rw=-bc+rs+self.bt*ca\n",
    "                cd={\"bic\":bc,\"rsparse\":rs,\"cf_acc\":ca,\"reward\":rw,\"sems\":sms,\"topo\":to};self._cp(ky,cd)\n",
    "            else:rw=float(cd[\"reward\"])\n",
    "            inf={\"cyclic\":False,\"edges\":int(A.sum()),\"bic\":float(cd[\"bic\"]),\"cf_acc\":float(cd[\"cf_acc\"]),\"rsparse\":float(cd[\"rsparse\"])}\n",
    "        return self.z.copy(),float(rw),False,False,inf\n",
    "\n",
    "@torch.no_grad()\n",
    "def _cu(evs:List[_CEF],ags:List[PPO],zt:float=0.3)->None:\n",
    "    K=len(evs)\n",
    "    if K<=1:return\n",
    "    zs=np.stack([e.z for e in evs],axis=0);zb=zs.mean(axis=0)\n",
    "    for e in evs:e.z=(1.0-zt)*e.z+zt*zb\n",
    "    sds=[a.policy.state_dict()for a in ags];avs={}\n",
    "    for k in sds[0].keys():ts=[sd[k].detach().clone()for sd in sds];avs[k]=torch.stack(ts,dim=0).mean(dim=0)\n",
    "    for ag in ags:\n",
    "        sd=ag.policy.state_dict()\n",
    "        for k,v in sd.items():sd[k]=(1.0-zt)*v+zt*avs[k]\n",
    "        ag.policy.load_state_dict(sd)\n",
    "\n",
    "@dataclass\n",
    "class _TC:\n",
    "    total_timesteps:int=500_000;K:int=3;alpha:float=0.02;beta:float=10.0;zeta:float=0.3;tau:float=0.5;z_clip:float=10.0;action_clip:float=0.5\n",
    "    ppo_lr:float=3e-4;n_steps:int=2048;batch_size:int=256;n_epochs:int=10;gamma:float=0.99;gae_lambda:float=0.95;clip_range:float=0.2;ent_coef:float=0.01;vf_coef:float=0.5;max_grad_norm:float=0.5\n",
    "    eta_mask:float=5e-4;lambda_prior:float=0.15;nmask:int=10;mask_update_freq:int=5000;consensus_interval:int=5000;eval_interval:int=25_000;seed:int=42\n",
    "    cf_max_samples:Optional[int]=None;cf_max_values_per_feature:Optional[int]=None\n",
    "\n",
    "def _train(dp:str,fc:Optional[List[str]]=None,cfg:_TC=_TC()):\n",
    "    _s(cfg.seed);dv=torch.device(\"cuda\"if torch.cuda.is_available()else\"cpu\")\n",
    "    dt,enc,fn,mt=_lpp(dp,feature_cols=fc);d=mt[\"d\"];C,E=_bdc(fn,d)\n",
    "    mm=_LDM(d,C,E,sd=cfg.seed).to(dv);mo=optim.Adam(mm.parameters(),lr=cfg.eta_mask)\n",
    "    cfc=_CFC(max_samples=cfg.cf_max_samples,max_values_per_feature=cfg.cf_max_values_per_feature,seed=cfg.seed)\n",
    "    evs:List[_CEF]=[];ags:List[PPO]=[]\n",
    "    for k in range(cfg.K):\n",
    "        ev=_CEF(d=d,dc=dt,mm=mm,al=cfg.alpha,bt=cfg.beta,tau=cfg.tau,zc=cfg.z_clip,ac=cfg.action_clip,cfc=cfc,sd=cfg.seed+k);evs.append(ev)\n",
    "        ag=PPO(policy=\"MlpPolicy\",env=ev,learning_rate=cfg.ppo_lr,n_steps=cfg.n_steps,batch_size=cfg.batch_size,n_epochs=cfg.n_epochs,gamma=cfg.gamma,gae_lambda=cfg.gae_lambda,clip_range=cfg.clip_range,ent_coef=cfg.ent_coef,vf_coef=cfg.vf_coef,max_grad_norm=cfg.max_grad_norm,verbose=0,device=dv);ags.append(ag)\n",
    "    hs=[];t=0;st=time.time();ch=cfg.n_steps\n",
    "    while t<cfg.total_timesteps:\n",
    "        for ag in ags:ag.learn(total_timesteps=ch,reset_num_timesteps=False,progress_bar=False)\n",
    "        t+=ch\n",
    "        if(t%cfg.mask_update_freq)==0:\n",
    "            for _ in range(cfg.nmask):mo.zero_grad(set_to_none=True);ls=mm._muo(cfg.lambda_prior);ls.backward();mm._zfg();mo.step()\n",
    "        if(t%cfg.consensus_interval)==0:_cu(evs,ags,zt=cfg.zeta)\n",
    "        if(t%cfg.eval_interval)==0:\n",
    "            with torch.no_grad():M=mm._mr().detach().cpu().numpy()\n",
    "            dgs=[];rws=[];eds=[];cfs=[];bcs=[];cy=0\n",
    "            for ev in evs:\n",
    "                As=_v2ds(ev.z,M,d);A=_ba(As,tau=cfg.tau)\n",
    "                if _ic(A):cy+=1;continue\n",
    "                bc,sms,to=_cbas(A,dt);ca=_cca(A,dt,sms,to,cfg=cfc);rw=-bc+(-cfg.alpha*float(A.sum()))+cfg.beta*ca\n",
    "                dgs.append(A);rws.append(rw);eds.append(float(A.sum()));cfs.append(ca);bcs.append(bc)\n",
    "            dr=1.0-(cy/cfg.K);ar=float(np.mean(rws))if rws else-1e6;ae=float(np.mean(eds))if eds else 0.0;ac=float(np.mean(cfs))if cfs else 0.0;ab=float(np.mean(bcs))if bcs else float(\"inf\")\n",
    "            hs.append([t,ar,ae,dr,ab,ac]);em=(time.time()-st)/60.0\n",
    "            print(f\"t={t:7d} | {em:6.1f}m | R={ar:10.2f} | edges={ae:6.1f} | DAG%={dr*100:5.1f} | BIC={ab:10.1f} | CF={ac:6.3f}\")\n",
    "    with torch.no_grad():M=mm._mr().detach().cpu().numpy()\n",
    "    dgs=[]\n",
    "    for ev in evs:\n",
    "        As=_v2ds(ev.z,M,d);A=_ba(As,tau=cfg.tau)\n",
    "        if not _ic(A):dgs.append(A.astype(np.int8))\n",
    "    if not dgs:Ac=np.zeros((d,d),dtype=np.int8)\n",
    "    else:\n",
    "        Am=np.mean(np.stack(dgs,axis=0),axis=0);Ac=(Am>0.5).astype(np.int8);np.fill_diagonal(Ac,0)\n",
    "        G=nx.DiGraph(Ac)\n",
    "        while not nx.is_directed_acyclic_graph(G):\n",
    "            cl=nx.find_cycle(G,orientation=\"original\");mw,me=1e9,None\n",
    "            for u,v,_ in cl:\n",
    "                w=Am[u,v]\n",
    "                if w<mw:mw,me=w,(u,v)\n",
    "            if me is None:break\n",
    "            u,v=me;Ac[u,v]=0;G.remove_edge(u,v)\n",
    "    if not _ic(Ac):bc,sms,to=_cbas(Ac,dt);ca=_cca(Ac,dt,sms,to,cfg=cfc);at=_ccfa(Ac,dt,enc,fn,sms,to,cfg=cfc)\n",
    "    else:bc,ca=float(\"inf\"),0.0;at=np.zeros((d-1,len(enc[-1].classes_)),dtype=np.float32)\n",
    "    return{\"A_consensus\":Ac,\"history\":np.array(hs,dtype=np.float64),\"feature_names\":fn,\"target_classes\":enc[-1].classes_.tolist(),\"metadata\":mt,\"mask_state_dict\":mm.state_dict(),\"bic\":bc,\"cf_acc\":ca,\"attribution\":at}\n",
    "\n",
    "def _sv(rs:Dict,od:str=\".\"):\n",
    "    os.makedirs(od,exist_ok=True);A=rs[\"A_consensus\"];np.savetxt(os.path.join(od,\"adjacency_matrix.csv\"),A,delimiter=\",\",fmt=\"%d\");np.save(os.path.join(od,\"adjacency_matrix.npy\"),A)\n",
    "    ht=rs[\"history\"]\n",
    "    if ht.size:pd.DataFrame(ht,columns=[\"t\",\"reward\",\"edges\",\"dag_rate\",\"bic\",\"cf_acc\"]).to_csv(os.path.join(od,\"training_history.csv\"),index=False)\n",
    "    torch.save(rs[\"mask_state_dict\"],os.path.join(od,\"domain_mask.pt\"));at=rs[\"attribution\"]\n",
    "    if at is not None:pd.DataFrame(at,index=rs[\"feature_names\"],columns=rs[\"target_classes\"]).to_csv(os.path.join(od,\"attribution_matrix.csv\"))\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    fc=[\"Defect Code\",\"Manufacturing Site\",\"Part Supplier\",\"Process\",\"Production Stage\",\"Product Series\",\"Product Type\",\"SubProduct Type\",\"Machine Name\",\"Defect Category\",\"Original Production Stage\",\"Destination Stage\",\"Memory Type\",\"Memory Size\",\"Memory SubType\",\"Disposition\",\"Defect Group\"]\n",
    "    cfg=_TC(total_timesteps=500_000,mask_update_freq=5000,consensus_interval=5000,eval_interval=25_000,cf_max_samples=None,cf_max_values_per_feature=None)\n",
    "    rs=_train(\"rough2.xlsx\",feature_cols=fc,cfg=cfg);print(\"\\nDone.\");print(\"Consensus edges:\",int(rs[\"A_consensus\"].sum()));print(\"Consensus cyclic?:\",_ic(rs[\"A_consensus\"]));print(\"Consensus BIC:\",rs[\"bic\"]);print(\"Consensus CF_Acc:\",rs[\"cf_acc\"]);_sv(rs,od=\"cf_marl_outputs\");print(\"Saved outputs to ./cf_marl_outputs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
